{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目标列: ['AUC', 'F1 Score', 'Accuracy', 'Sensitivity', 'Specificity']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:200: RuntimeWarning: invalid value encountered in cast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |      100 |      1 |             - |             -\n",
      "     2 |      200 |      2 |  1.0000000000 |         ideal\n",
      "     3 |      300 |      3 |  2.4140741772 |         ideal\n",
      "     4 |      400 |      3 |  0.8234676571 |         ideal\n",
      "     5 |      500 |      4 |  0.7596380617 |         ideal\n",
      "     6 |      600 |      4 |  0.000000E+00 |             f\n",
      "     7 |      700 |      1 |  0.0158039091 |         ideal\n",
      "     8 |      800 |      1 |  0.000000E+00 |             f\n",
      "     9 |      900 |      1 |  0.000000E+00 |             f\n",
      "    10 |     1000 |      1 |  0.000000E+00 |             f\n",
      "    11 |     1100 |      1 |  0.000000E+00 |             f\n",
      "    12 |     1200 |      1 |  0.000000E+00 |             f\n",
      "    13 |     1300 |      1 |  0.000000E+00 |             f\n",
      "    14 |     1400 |      1 |  0.000000E+00 |             f\n",
      "    15 |     1500 |      1 |  0.000000E+00 |             f\n",
      "    16 |     1600 |      1 |  0.000000E+00 |             f\n",
      "    17 |     1700 |      1 |  0.000000E+00 |             f\n",
      "    18 |     1800 |      1 |  0.000000E+00 |             f\n",
      "    19 |     1900 |      1 |  0.000000E+00 |             f\n",
      "    20 |     2000 |      1 |  0.000000E+00 |             f\n",
      "    21 |     2100 |      1 |  0.000000E+00 |             f\n",
      "    22 |     2200 |      1 |  0.000000E+00 |             f\n",
      "    23 |     2300 |      1 |  0.000000E+00 |             f\n",
      "    24 |     2400 |      1 |  0.000000E+00 |             f\n",
      "    25 |     2500 |      1 |  0.000000E+00 |             f\n",
      "    26 |     2600 |      1 |  0.000000E+00 |             f\n",
      "    27 |     2700 |      1 |  0.000000E+00 |             f\n",
      "    28 |     2800 |      1 |  0.000000E+00 |             f\n",
      "    29 |     2900 |      1 |  0.000000E+00 |             f\n",
      "    30 |     3000 |      1 |  0.000000E+00 |             f\n",
      "    31 |     3100 |      1 |  0.000000E+00 |             f\n",
      "    32 |     3200 |      1 |  0.000000E+00 |             f\n",
      "    33 |     3300 |      1 |  0.000000E+00 |             f\n",
      "    34 |     3400 |      1 |  0.000000E+00 |             f\n",
      "    35 |     3500 |      1 |  0.000000E+00 |             f\n",
      "    36 |     3600 |      1 |  0.000000E+00 |             f\n",
      "    37 |     3700 |      1 |  0.000000E+00 |             f\n",
      "    38 |     3800 |      1 |  0.000000E+00 |             f\n",
      "    39 |     3900 |      1 |  0.000000E+00 |             f\n",
      "    40 |     4000 |      1 |  0.000000E+00 |             f\n",
      "    41 |     4100 |      1 |  0.000000E+00 |             f\n",
      "    42 |     4200 |      1 |  0.000000E+00 |             f\n",
      "    43 |     4300 |      1 |  0.000000E+00 |             f\n",
      "    44 |     4400 |      1 |  0.000000E+00 |             f\n",
      "    45 |     4500 |      1 |  0.000000E+00 |             f\n",
      "    46 |     4600 |      1 |  0.000000E+00 |             f\n",
      "    47 |     4700 |      1 |  0.000000E+00 |             f\n",
      "    48 |     4800 |      1 |  0.000000E+00 |             f\n",
      "    49 |     4900 |      1 |  0.000000E+00 |             f\n",
      "    50 |     5000 |      1 |  0.000000E+00 |             f\n",
      "    51 |     5100 |      1 |  0.000000E+00 |             f\n",
      "    52 |     5200 |      1 |  0.000000E+00 |             f\n",
      "    53 |     5300 |      1 |  0.000000E+00 |             f\n",
      "    54 |     5400 |      1 |  0.000000E+00 |             f\n",
      "    55 |     5500 |      1 |  0.000000E+00 |             f\n",
      "    56 |     5600 |      1 |  0.000000E+00 |             f\n",
      "    57 |     5700 |      1 |  0.000000E+00 |             f\n",
      "    58 |     5800 |      1 |  0.000000E+00 |             f\n",
      "    59 |     5900 |      1 |  0.000000E+00 |             f\n",
      "    60 |     6000 |      1 |  0.000000E+00 |             f\n",
      "    61 |     6100 |      1 |  0.000000E+00 |             f\n",
      "    62 |     6200 |      1 |  0.000000E+00 |             f\n",
      "    63 |     6300 |      1 |  0.000000E+00 |             f\n",
      "    64 |     6400 |      1 |  0.000000E+00 |             f\n",
      "    65 |     6500 |      1 |  0.000000E+00 |             f\n",
      "    66 |     6600 |      1 |  0.000000E+00 |             f\n",
      "    67 |     6700 |      1 |  0.000000E+00 |             f\n",
      "    68 |     6800 |      1 |  0.000000E+00 |             f\n",
      "    69 |     6900 |      1 |  0.000000E+00 |             f\n",
      "    70 |     7000 |      1 |  0.000000E+00 |             f\n",
      "    71 |     7100 |      1 |  0.000000E+00 |             f\n",
      "    72 |     7200 |      1 |  0.000000E+00 |             f\n",
      "    73 |     7300 |      1 |  0.000000E+00 |             f\n",
      "    74 |     7400 |      1 |  0.000000E+00 |             f\n",
      "    75 |     7500 |      1 |  0.000000E+00 |             f\n",
      "    76 |     7600 |      1 |  0.000000E+00 |             f\n",
      "    77 |     7700 |      1 |  0.000000E+00 |             f\n",
      "    78 |     7800 |      1 |  0.000000E+00 |             f\n",
      "    79 |     7900 |      1 |  0.000000E+00 |             f\n",
      "    80 |     8000 |      1 |  0.000000E+00 |             f\n",
      "    81 |     8100 |      1 |  0.000000E+00 |             f\n",
      "    82 |     8200 |      1 |  0.000000E+00 |             f\n",
      "    83 |     8300 |      1 |  0.000000E+00 |             f\n",
      "    84 |     8400 |      1 |  0.000000E+00 |             f\n",
      "    85 |     8500 |      1 |  0.000000E+00 |             f\n",
      "    86 |     8600 |      1 |  0.000000E+00 |             f\n",
      "    87 |     8700 |      1 |  0.000000E+00 |             f\n",
      "    88 |     8800 |      1 |  0.000000E+00 |             f\n",
      "    89 |     8900 |      1 |  0.000000E+00 |             f\n",
      "    90 |     9000 |      1 |  0.000000E+00 |             f\n",
      "    91 |     9100 |      1 |  0.000000E+00 |             f\n",
      "    92 |     9200 |      1 |  0.000000E+00 |             f\n",
      "    93 |     9300 |      1 |  0.000000E+00 |             f\n",
      "    94 |     9400 |      1 |  0.000000E+00 |             f\n",
      "    95 |     9500 |      1 |  0.000000E+00 |             f\n",
      "    96 |     9600 |      1 |  0.000000E+00 |             f\n",
      "    97 |     9700 |      1 |  0.000000E+00 |             f\n",
      "    98 |     9800 |      1 |  0.000000E+00 |             f\n",
      "    99 |     9900 |      1 |  0.000000E+00 |             f\n",
      "   100 |    10000 |      1 |  0.000000E+00 |             f\n",
      "Solution 1: Selected model type: AdaBoost\n",
      "Best solution selected model type: AdaBoost\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymoo.core.problem import ElementwiseProblem\n",
    "from pymoo.algorithms.moo.nsga3 import NSGA3\n",
    "from pymoo.util.ref_dirs import get_reference_directions\n",
    "from pymoo.optimize import minimize\n",
    "\n",
    "# 文件路径\n",
    "mdd_path = 'C:/Users/74101/Desktop/成人抑郁症/result/12.5/result/mdd_all/cv_allrna.csv'\n",
    "bd_path = 'C:/Users/74101/Desktop/成人抑郁症/result/10.22/result/all_bp/cv_allrna.csv'\n",
    "hc_path = 'C:/Users/74101/Desktop/成人抑郁症/result/10.22/result/all_mdd/cv_allrna.csv'\n",
    "hamd_path = 'C:/Users/74101/Desktop/成人抑郁症/result/10.22/result/all_hmad/cv_allrna.csv'\n",
    "\n",
    "# 读取每个文件\n",
    "MDD = pd.read_csv(mdd_path)\n",
    "BD = pd.read_csv(bd_path)\n",
    "HC = pd.read_csv(hc_path)\n",
    "HAMD = pd.read_csv(hamd_path)\n",
    "\n",
    "# 按 'Model' 分组并计算均值\n",
    "MDD = MDD.groupby('Model').mean(numeric_only=True).reset_index()\n",
    "BD = BD.groupby('Model').mean(numeric_only=True).reset_index()\n",
    "HC = HC.groupby('Model').mean(numeric_only=True).reset_index()\n",
    "HAMD = HAMD.groupby('Model').mean(numeric_only=True).reset_index()\n",
    "\n",
    "# 数据整理为字典\n",
    "data_dict = {\n",
    "    \"MDD\": MDD,\n",
    "    \"BD\": BD,\n",
    "    \"HC\": HC,\n",
    "    \"HAMD\": HAMD\n",
    "}\n",
    "\n",
    "# 动态提取性能指标列（除去 \"Model\"）\n",
    "objectives = [\"AUC\", \"F1 Score\", \"Accuracy\", 'Sensitivity', 'Specificity']\n",
    "print(\"目标列:\", objectives)\n",
    "\n",
    "# 获取所有模型类型\n",
    "all_models = list(MDD[\"Model\"].unique())\n",
    "\n",
    "# 定义多目标优化问题\n",
    "class MultiObjectiveModelSelection(ElementwiseProblem):\n",
    "    def __init__(self, data_dict, all_models, objectives):\n",
    "        self.data_dict = data_dict\n",
    "        self.all_models = all_models\n",
    "        self.objectives = objectives\n",
    "        n_var = len(all_models)  # 决策变量数量（模型数量）\n",
    "        n_obj = len(objectives)  # 目标数量（性能指标数量）\n",
    "        super().__init__(n_var=n_var, n_obj=n_obj, xl=0, xu=1, type_var=float)  # 修改为float\n",
    "\n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        # 将 x 转换为二进制选择\n",
    "        x_binary = np.round(x).astype(int)\n",
    "        selected_models = [self.all_models[i] for i, selected in enumerate(x_binary) if selected == 1]\n",
    "        \n",
    "        if not selected_models:\n",
    "            # 如果未选中任何模型，返回性能值最差的情况\n",
    "            out[\"F\"] = [1.0] * len(self.objectives)  # 所有目标设为 1（最差性能）\n",
    "            return\n",
    "        \n",
    "        scores = []\n",
    "        for obj in self.objectives:\n",
    "            obj_scores = []\n",
    "            for dataset in self.data_dict.values():\n",
    "                # 筛选当前数据集中的选中模型\n",
    "                selected_data = dataset[dataset[\"Model\"].isin(selected_models)]\n",
    "                if selected_data.empty:\n",
    "                    obj_scores.append(1.0)  # 若无选中模型，目标值为 1\n",
    "                else:\n",
    "                    # 计算选中模型的平均性能并取负值（目标最小化）\n",
    "                    obj_scores.append(-selected_data[obj].mean())\n",
    "            scores.append(np.mean(obj_scores))  # 对所有数据集取平均\n",
    "        out[\"F\"] = scores\n",
    "\n",
    "# 定义优化问题\n",
    "problem = MultiObjectiveModelSelection(data_dict, all_models, objectives)\n",
    "\n",
    "# 设置参考方向和算法\n",
    "ref_dirs = get_reference_directions(\"das-dennis\", n_dim=len(objectives), n_partitions=3)\n",
    "algorithm = NSGA3(pop_size=100, ref_dirs=ref_dirs)\n",
    "\n",
    "# 执行优化\n",
    "res = minimize(problem, algorithm, ('n_gen', 100), seed=42, verbose=True)\n",
    "\n",
    "# 解析结果\n",
    "for i, solution in enumerate(res.X):\n",
    "    # 找到最大概率的模型索引\n",
    "    max_index = np.argmax(solution)\n",
    "    selected_model = all_models[max_index]\n",
    "    print(f\"Solution {i + 1}: Selected model type: {selected_model}\")\n",
    "\n",
    "# 如果你只需要最优解的选择，可以使用：\n",
    "optimal_solution = res.X[0]\n",
    "max_index = np.argmax(optimal_solution)\n",
    "selected_model = all_models[max_index]\n",
    "print(f\"Best solution selected model type: {selected_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 外部筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\74101\\AppData\\Local\\Temp\\ipykernel_25620\\3038364813.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['group'] = filtered_data['group'].replace(replace_map)\n",
      "C:\\Users\\74101\\AppData\\Local\\Temp\\ipykernel_25620\\3038364813.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['group'] = filtered_data['group'].replace(replace_map)\n",
      "C:\\Users\\74101\\AppData\\Local\\Temp\\ipykernel_25620\\3038364813.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['group'] = filtered_data['group'].replace(replace_map)\n",
      "C:\\Users\\74101\\AppData\\Local\\Temp\\ipykernel_25620\\3038364813.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['group'] = filtered_data['group'].replace(replace_map)\n",
      "C:\\Users\\74101\\AppData\\Local\\Temp\\ipykernel_25620\\3038364813.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['group'] = filtered_data['group'].replace(replace_map)\n",
      "C:\\Users\\74101\\AppData\\Local\\Temp\\ipykernel_25620\\3038364813.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['group'] = filtered_data['group'].replace(replace_map)\n",
      "C:\\Users\\74101\\AppData\\Local\\Temp\\ipykernel_25620\\3038364813.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['group'] = filtered_data['group'].replace(replace_map)\n",
      "C:\\Users\\74101\\AppData\\Local\\Temp\\ipykernel_25620\\3038364813.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['group'] = filtered_data['group'].replace(replace_map)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 导入数据\n",
    "data_row = pd.read_csv('C:/Users/74101/Desktop/成人抑郁症/data/all.csv', encoding='GBK')\n",
    "\n",
    "# 数据分组\n",
    "train_data = data_row[data_row.iloc[:, 0].str.startswith('train')]\n",
    "test_data = data_row[data_row.iloc[:, 0].str.startswith('test')]\n",
    "\n",
    "# 定义分组逻辑的通用函数\n",
    "def process_data(data, group_filter, replace_map, drop_columns):\n",
    "    \"\"\"\n",
    "    按组过滤和处理数据。\n",
    "\n",
    "    :param data: 原始数据\n",
    "    :param group_filter: 筛选组的条件列表\n",
    "    :param replace_map: group 列的替换映射\n",
    "    :param drop_columns: 需要移除的列\n",
    "    :return: 处理后的特征和标签\n",
    "    \"\"\"\n",
    "    filtered_data = data[data['group'].isin(group_filter)]\n",
    "    filtered_data['group'] = filtered_data['group'].replace(replace_map)\n",
    "    group = filtered_data['group']\n",
    "    features = filtered_data.drop(columns=drop_columns)\n",
    "\n",
    "    # 替换所有NaN为0\n",
    "    features = features.fillna(0)\n",
    "\n",
    "    return features, group\n",
    "\n",
    "# 公共需要移除的列\n",
    "common_drop_columns = ['allRNA', 'Hospital', 'Sample_id', 'Company', 'Batch', 'group', 'Age', 'HAMD', 'Diagnosis', 'Gender']\n",
    "\n",
    "# MDD 数据\n",
    "train_mdd_feature, train_mdd_group = process_data(\n",
    "    train_data, group_filter=[3, 1, 0], replace_map={3: 0}, drop_columns=common_drop_columns)\n",
    "test_mdd_feature, test_mdd_group = process_data(\n",
    "    test_data, group_filter=[3, 1, 0], replace_map={3: 0}, drop_columns=common_drop_columns)\n",
    "\n",
    "# BD 数据\n",
    "train_bd_feature, train_bd_group = process_data(\n",
    "    train_data, group_filter=[2, 1], replace_map={2: 0}, drop_columns=common_drop_columns)\n",
    "test_bd_feature, test_bd_group = process_data(\n",
    "    test_data, group_filter=[2, 1], replace_map={2: 0}, drop_columns=common_drop_columns)\n",
    "\n",
    "# HC 数据\n",
    "train_hc_feature, train_hc_group = process_data(\n",
    "    train_data, group_filter=[1, 0], replace_map={}, drop_columns=common_drop_columns)\n",
    "test_hc_feature, test_hc_group = process_data(\n",
    "    test_data, group_filter=[1, 0], replace_map={}, drop_columns=common_drop_columns)\n",
    "\n",
    "# Other 数据\n",
    "train_other_feature, train_other_group = process_data(\n",
    "    train_data, group_filter=[3, 1], replace_map={3: 0}, drop_columns=common_drop_columns)\n",
    "test_other_feature, test_other_group = process_data(\n",
    "    test_data, group_filter=[3, 1], replace_map={3: 0}, drop_columns=common_drop_columns)\n",
    "\n",
    "# HAMD 数据\n",
    "train_hamd = train_data[train_data['group'].isin([1, 0])]\n",
    "test_hamd = test_data[test_data['group'].isin([1, 0])]\n",
    "\n",
    "# 选择 'HAMD' 非空的行\n",
    "train_hamd_filtered = train_hamd[train_hamd['HAMD'].notna()].copy()\n",
    "test_hamd_filtered = test_hamd[test_hamd['HAMD'].notna()].copy()\n",
    "\n",
    "# 将 'HAMD' 列的值按照范围分组：8-20 的标注为 0，超过 20 的标注为 1\n",
    "train_hamd_filtered['HAMD_Group'] = train_hamd_filtered['HAMD'].apply(lambda x: 0 if x < 20 else 1)\n",
    "test_hamd_filtered['HAMD_Group'] = test_hamd_filtered['HAMD'].apply(lambda x: 0 if x < 20 else 1)\n",
    "\n",
    "# 提取特征和分组信息\n",
    "train_hamd_feature = train_hamd_filtered.drop(columns=[\n",
    "    'allRNA', 'Hospital', 'Sample_id', 'Company', 'Batch', 'group', 'Age', 'HAMD', 'Diagnosis', 'Gender', 'HAMD_Group'\n",
    "])\n",
    "train_hamd_group = train_hamd_filtered['HAMD_Group']\n",
    "\n",
    "test_hamd_feature = test_hamd_filtered.drop(columns=[\n",
    "    'allRNA', 'Hospital', 'Sample_id', 'Company', 'Batch', 'group', 'Age', 'HAMD', 'Diagnosis', 'Gender', 'HAMD_Group'\n",
    "])\n",
    "test_hamd_group = test_hamd_filtered['HAMD_Group']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost model (MDD) loaded successfully.\n",
      "CatBoost model (MDD) loaded successfully.\n",
      "GBDT model (MDD) loaded successfully.\n",
      "LightGBM model (MDD) loaded successfully.\n",
      "Logistic Regression model (MDD) loaded successfully.\n",
      "MLP model (MDD) loaded successfully.\n",
      "Random Forest model (MDD) loaded successfully.\n",
      "SVM model (MDD) loaded successfully.\n",
      "XGBoost model (MDD) loaded successfully.\n",
      "AdaBoost model (BD) loaded successfully.\n",
      "CatBoost model (BD) loaded successfully.\n",
      "GBDT model (BD) loaded successfully.\n",
      "LightGBM model (BD) loaded successfully.\n",
      "Logistic Regression model (BD) loaded successfully.\n",
      "MLP model (BD) loaded successfully.\n",
      "Random Forest model (BD) loaded successfully.\n",
      "SVM model (BD) loaded successfully.\n",
      "XGBoost model (BD) loaded successfully.\n",
      "AdaBoost model (HC) loaded successfully.\n",
      "CatBoost model (HC) loaded successfully.\n",
      "GBDT model (HC) loaded successfully.\n",
      "LightGBM model (HC) loaded successfully.\n",
      "Logistic Regression model (HC) loaded successfully.\n",
      "MLP model (HC) loaded successfully.\n",
      "Random Forest model (HC) loaded successfully.\n",
      "SVM model (HC) loaded successfully.\n",
      "XGBoost model (HC) loaded successfully.\n",
      "AdaBoost model (Other) loaded successfully.\n",
      "CatBoost model (Other) loaded successfully.\n",
      "GBDT model (Other) loaded successfully.\n",
      "LightGBM model (Other) loaded successfully.\n",
      "Logistic Regression model (Other) loaded successfully.\n",
      "MLP model (Other) loaded successfully.\n",
      "Random Forest model (Other) loaded successfully.\n",
      "SVM model (Other) loaded successfully.\n",
      "XGBoost model (Other) loaded successfully.\n",
      "AdaBoost model (HAMD) loaded successfully.\n",
      "CatBoost model (HAMD) loaded successfully.\n",
      "GBDT model (HAMD) loaded successfully.\n",
      "LightGBM model (HAMD) loaded successfully.\n",
      "Logistic Regression model (HAMD) loaded successfully.\n",
      "MLP model (HAMD) loaded successfully.\n",
      "Random Forest model (HAMD) loaded successfully.\n",
      "SVM model (HAMD) loaded successfully.\n",
      "XGBoost model (HAMD) loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# 导入库\n",
    "import joblib\n",
    "\n",
    "# 模型名称列表\n",
    "model_names = ['AdaBoost', 'CatBoost', 'GBDT', 'LightGBM', 'Logistic Regression', \n",
    "               'MLP', 'Random Forest', 'SVM', 'XGBoost']\n",
    "\n",
    "# MDD 模型\n",
    "mdd_models = {}\n",
    "for name in model_names:\n",
    "    model_path = f'C:/Users/74101/Desktop/成人抑郁症/result/12.5/result/mdd_all/model/{name}_model.pkl'\n",
    "    try:\n",
    "        mdd_models[name] = joblib.load(model_path)\n",
    "        print(f\"{name} model (MDD) loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {name} model (MDD): {e}\")\n",
    "\n",
    "# BD 模型\n",
    "bd_models = {}\n",
    "for name in model_names:\n",
    "    model_path = f'C:/Users/74101/Desktop/成人抑郁症/result/10.22/result/all_bp/model/{name}_model.pkl'\n",
    "    try:\n",
    "        bd_models[name] = joblib.load(model_path)\n",
    "        print(f\"{name} model (BD) loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {name} model (BD): {e}\")\n",
    "\n",
    "# HC 模型\n",
    "hc_models = {}\n",
    "for name in model_names:\n",
    "    model_path = f'C:/Users/74101/Desktop/成人抑郁症/result/10.22/result/all_mdd/model/{name}_model.pkl'\n",
    "    try:\n",
    "        hc_models[name] = joblib.load(model_path)\n",
    "        print(f\"{name} model (HC) loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {name} model (HC): {e}\")\n",
    "\n",
    "# Other 模型\n",
    "other_models = {}\n",
    "for name in model_names:\n",
    "    model_path = f'C:/Users/74101/Desktop/成人抑郁症/result/12.5/result/other_all/model/{name}_model.pkl'\n",
    "    try:\n",
    "        other_models[name] = joblib.load(model_path)\n",
    "        print(f\"{name} model (Other) loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {name} model (Other): {e}\")\n",
    "\n",
    "# HAMD 模型\n",
    "\n",
    "hamd_models = {}\n",
    "for name in model_names:\n",
    "    model_path = f'C:/Users/74101/Desktop/成人抑郁症/result/10.22/result/all_hmad/model/{name}_model.pkl'\n",
    "    try:\n",
    "        hamd_models[name] = joblib.load(model_path)\n",
    "        print(f\"{name} model (HAMD) loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {name} model (HAMD): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"MDD\": mdd_models,\n",
    "    \"BD\": bd_models,\n",
    "    \"HC\": hc_models,\n",
    "    \"Other\": other_models,\n",
    "    \"HAMD\": hamd_models\n",
    "}\n",
    "\n",
    "dataset = {\n",
    "    \"MDD\": (test_mdd_feature, test_mdd_group),\n",
    "    \"BD\": (test_bd_feature, test_bd_group),\n",
    "    \"HC\": (test_hc_feature, test_hc_group),\n",
    "    \"Other\": (test_other_feature, test_other_group),\n",
    "    \"HAMD\": (test_hamd_feature, test_hamd_group)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dataset                Model       AUC  Accuracy  Sensitivity (SN)  \\\n",
      "0      MDD             AdaBoost  0.945840  0.877551          0.820144   \n",
      "1      MDD             CatBoost  0.944957  0.877551          0.784173   \n",
      "2      MDD                 GBDT  0.948486  0.906122          0.856115   \n",
      "3      MDD             LightGBM  0.960364  0.877551          0.820144   \n",
      "4      MDD  Logistic Regression  0.917402  0.828571          0.748201   \n",
      "5      MDD                  MLP  0.911565  0.783673          0.669065   \n",
      "6      MDD        Random Forest  0.936032  0.885714          0.805755   \n",
      "7      MDD                  SVM  0.945161  0.853061          0.769784   \n",
      "8      MDD              XGBoost  0.934437  0.873469          0.812950   \n",
      "9       BD             AdaBoost  0.738530  0.710000          0.856115   \n",
      "10      BD             CatBoost  0.860715  0.695000          0.992806   \n",
      "11      BD                 GBDT  0.691709  0.615000          0.877698   \n",
      "12      BD             LightGBM  0.816488  0.695000          1.000000   \n",
      "13      BD  Logistic Regression  0.910603  0.865000          0.920863   \n",
      "14      BD                  MLP  0.909777  0.870000          0.899281   \n",
      "15      BD        Random Forest  0.911428  0.765000          0.992806   \n",
      "16      BD                  SVM  0.923340  0.840000          0.956835   \n",
      "17      BD              XGBoost  0.806227  0.675000          0.964029   \n",
      "18      HC             AdaBoost  0.946853  0.899543          0.841727   \n",
      "19      HC             CatBoost  0.957554  0.821918          0.719424   \n",
      "20      HC                 GBDT  0.977788  0.908676          0.863309   \n",
      "21      HC             LightGBM  0.956205  0.890411          0.834532   \n",
      "22      HC  Logistic Regression  0.911781  0.799087          0.726619   \n",
      "23      HC                  MLP  0.895144  0.753425          0.618705   \n",
      "24      HC        Random Forest  0.969694  0.858447          0.776978   \n",
      "25      HC                  SVM  0.931655  0.853881          0.784173   \n",
      "26      HC              XGBoost  0.955576  0.867580          0.791367   \n",
      "27   Other             AdaBoost  0.819452  0.945455          1.000000   \n",
      "28   Other             CatBoost  0.883509  0.842424          1.000000   \n",
      "29   Other                 GBDT  0.969286  0.842424          1.000000   \n",
      "30   Other             LightGBM  0.932761  0.878788          1.000000   \n",
      "31   Other  Logistic Regression  0.909519  0.921212          0.971223   \n",
      "32   Other                  MLP  0.830105  0.854545          0.985612   \n",
      "33   Other        Random Forest  0.949087  0.842424          1.000000   \n",
      "34   Other                  SVM  0.780852  0.842424          1.000000   \n",
      "35   Other              XGBoost  0.930825  0.842424          1.000000   \n",
      "36    HAMD             AdaBoost  0.750917  0.673267          0.803571   \n",
      "37    HAMD             CatBoost  0.758562  0.673267          0.696429   \n",
      "38    HAMD                 GBDT  0.758806  0.673267          0.660714   \n",
      "39    HAMD             LightGBM  0.744863  0.663366          0.750000   \n",
      "40    HAMD  Logistic Regression  0.735201  0.673267          0.696429   \n",
      "41    HAMD                  MLP  0.687439  0.628713          0.589286   \n",
      "42    HAMD        Random Forest  0.763209  0.648515          0.696429   \n",
      "43    HAMD                  SVM  0.687255  0.693069          0.642857   \n",
      "44    HAMD              XGBoost  0.752935  0.673267          0.732143   \n",
      "\n",
      "    Specificity (SP)  F1 Score     AUPRC  Log-Loss       MCC  \n",
      "0           0.952830  0.883721  0.962064  0.477767  0.766243  \n",
      "1           1.000000  0.879032  0.965833  0.380717  0.781789  \n",
      "2           0.971698  0.911877  0.969039  0.465927  0.820276  \n",
      "3           0.952830  0.883721  0.973314  0.679025  0.766243  \n",
      "4           0.933962  0.832000  0.936042  0.619350  0.678946  \n",
      "5           0.933962  0.778243  0.937233  0.785169  0.607873  \n",
      "6           0.990566  0.888889  0.959272  0.321007  0.791448  \n",
      "7           0.962264  0.856000  0.961372  0.353305  0.728595  \n",
      "8           0.952830  0.879377  0.960205  0.419690  0.759314  \n",
      "9           0.377049  0.804054  0.849198  1.978650  0.261307  \n",
      "10          0.016393  0.818991  0.941786  0.536718  0.042567  \n",
      "11          0.016393  0.760125  0.874326  0.867171 -0.170385  \n",
      "12          0.000000  0.820059  0.932731  0.670127  0.000000  \n",
      "13          0.737705  0.904594  0.929553  0.420837  0.675302  \n",
      "14          0.803279  0.905797  0.945519  0.414160  0.696345  \n",
      "15          0.245902  0.854489  0.962311  0.400093  0.405107  \n",
      "16          0.573770  0.892617  0.957132  0.342533  0.605136  \n",
      "17          0.016393  0.804805  0.931922  0.724733 -0.052840  \n",
      "18          1.000000  0.914062  0.973646  0.482944  0.812513  \n",
      "19          1.000000  0.836820  0.977997  0.398176  0.695447  \n",
      "20          0.987500  0.923077  0.988501  0.352445  0.823908  \n",
      "21          0.987500  0.906250  0.977998  0.795102  0.793502  \n",
      "22          0.925000  0.821138  0.953597  0.647550  0.627690  \n",
      "23          0.987500  0.761062  0.944080  1.173264  0.596520  \n",
      "24          1.000000  0.874494  0.984149  0.314021  0.748321  \n",
      "25          0.975000  0.872000  0.966540  0.408464  0.731172  \n",
      "26          1.000000  0.883534  0.977626  0.450460  0.762115  \n",
      "27          0.653846  0.968641  0.936715  0.333622  0.783636  \n",
      "28          0.000000  0.914474  0.977687  0.383342  0.000000  \n",
      "29          0.000000  0.914474  0.994481  0.468120  0.000000  \n",
      "30          0.230769  0.932886  0.987729  0.374157  0.449157  \n",
      "31          0.653846  0.954064  0.979723  0.267364  0.683331  \n",
      "32          0.153846  0.919463  0.960430  0.517069  0.271433  \n",
      "33          0.000000  0.914474  0.990739  0.279145  0.000000  \n",
      "34          0.000000  0.914474  0.958799  0.410457  0.000000  \n",
      "35          0.000000  0.914474  0.987187  0.375418  0.000000  \n",
      "36          0.623288  0.576923  0.478737  0.592127  0.382169  \n",
      "37          0.664384  0.541667  0.478589  0.966800  0.325730  \n",
      "38          0.678082  0.528571  0.472297  2.033741  0.307701  \n",
      "39          0.630137  0.552632  0.485046  0.695931  0.340739  \n",
      "40          0.664384  0.541667  0.454328  0.890854  0.325730  \n",
      "41          0.643836  0.468085  0.389112  1.035577  0.211373  \n",
      "42          0.630137  0.523490  0.480944  0.646323  0.293282  \n",
      "43          0.712329  0.537313  0.462322  0.675994  0.326564  \n",
      "44          0.650685  0.554054  0.469498  0.713499  0.344099  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, average_precision_score, log_loss, matthews_corrcoef\n",
    "import pandas as pd\n",
    "\n",
    "# 定义性能评估函数\n",
    "def evaluate_model_performance(models_dict, dataset):\n",
    "    \"\"\"\n",
    "    对模型在指定数据集上进行评估。\n",
    "    \n",
    "    :param models_dict: 字典，键为数据集名称，值为模型列表\n",
    "    :param dataset: 字典，键为数据集名称，值为 (特征, 标签)\n",
    "    :return: 各模型的性能结果\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for name, models in models_dict.items():\n",
    "        # 获取对应数据集\n",
    "        test_features, test_labels = dataset[name]\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            # 模型预测概率和类别\n",
    "            y_pred_prob = model.predict_proba(test_features)[:, 1]  # 获取正类的预测概率\n",
    "            y_pred = model.predict(test_features)  # 获取预测标签\n",
    "\n",
    "            # 计算各项指标\n",
    "            auc = roc_auc_score(test_labels, y_pred_prob)\n",
    "            acc = accuracy_score(test_labels, y_pred)\n",
    "            sn = recall_score(test_labels, y_pred)  # 敏感性 (召回率)\n",
    "            sp = specificity_score(test_labels, y_pred)  # 特异性\n",
    "            f1 = f1_score(test_labels, y_pred)\n",
    "            auprc = average_precision_score(test_labels, y_pred_prob)  # 精确率-召回率曲线下的面积\n",
    "            logloss = log_loss(test_labels, y_pred_prob)  # 对数损失\n",
    "            mcc = matthews_corrcoef(test_labels, y_pred)  # 马修斯相关系数\n",
    "\n",
    "            # 保存结果\n",
    "            results.append({\n",
    "                \"Dataset\": name,\n",
    "                \"Model\": model_name,\n",
    "                \"AUC\": auc,\n",
    "                \"Accuracy\": acc,\n",
    "                \"Sensitivity (SN)\": sn,\n",
    "                \"Specificity (SP)\": sp,\n",
    "                \"F1 Score\": f1,\n",
    "                \"AUPRC\": auprc,\n",
    "                \"Log-Loss\": logloss,\n",
    "                \"MCC\": mcc\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 定义计算特异性的辅助函数\n",
    "def specificity_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算特异性。\n",
    "    \n",
    "    :param y_true: 真实标签\n",
    "    :param y_pred: 预测标签\n",
    "    :return: 特异性\n",
    "    \"\"\"\n",
    "    tn, fp, _, _ = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "# 调用评估函数\n",
    "performance_results = evaluate_model_performance(models_dict, dataset)\n",
    "\n",
    "# 打印或保存结果\n",
    "print(performance_results)\n",
    "performance_results.to_csv('C:/Users/74101/Desktop/成人抑郁症/result/12.5/result/model_performance1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:200: RuntimeWarning: invalid value encountered in cast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |      210 |     10 |             - |             -\n",
      "     2 |      420 |     10 |  0.4781006623 |         ideal\n",
      "     3 |      630 |     12 |  0.0077704091 |         ideal\n",
      "     4 |      840 |     12 |  0.0301729347 |         ideal\n",
      "     5 |     1050 |     11 |  0.8258382451 |         nadir\n",
      "     6 |     1260 |     13 |  0.4385279408 |         nadir\n",
      "     7 |     1470 |     14 |  0.3357743947 |         nadir\n",
      "     8 |     1680 |     15 |  0.0075082749 |         ideal\n",
      "     9 |     1890 |     15 |  0.000000E+00 |             f\n",
      "    10 |     2100 |     15 |  0.000000E+00 |             f\n",
      "    11 |     2310 |     15 |  0.000000E+00 |             f\n",
      "    12 |     2520 |     15 |  0.000000E+00 |             f\n",
      "    13 |     2730 |     15 |  0.000000E+00 |             f\n",
      "    14 |     2940 |     15 |  0.000000E+00 |             f\n",
      "    15 |     3150 |     15 |  0.000000E+00 |             f\n",
      "    16 |     3360 |     15 |  0.000000E+00 |             f\n",
      "    17 |     3570 |     15 |  0.000000E+00 |             f\n",
      "    18 |     3780 |     15 |  0.000000E+00 |             f\n",
      "    19 |     3990 |     15 |  0.000000E+00 |             f\n",
      "    20 |     4200 |     15 |  0.000000E+00 |             f\n",
      "    21 |     4410 |     15 |  0.000000E+00 |             f\n",
      "    22 |     4620 |     15 |  0.000000E+00 |             f\n",
      "    23 |     4830 |     15 |  0.000000E+00 |             f\n",
      "    24 |     5040 |     15 |  0.000000E+00 |             f\n",
      "    25 |     5250 |     15 |  0.000000E+00 |             f\n",
      "    26 |     5460 |     15 |  0.000000E+00 |             f\n",
      "    27 |     5670 |     15 |  0.000000E+00 |             f\n",
      "    28 |     5880 |     15 |  0.000000E+00 |             f\n",
      "    29 |     6090 |     15 |  0.000000E+00 |             f\n",
      "    30 |     6300 |     15 |  0.000000E+00 |             f\n",
      "    31 |     6510 |     15 |  0.000000E+00 |             f\n",
      "    32 |     6720 |     15 |  0.000000E+00 |             f\n",
      "    33 |     6930 |     15 |  0.000000E+00 |             f\n",
      "    34 |     7140 |     15 |  0.000000E+00 |             f\n",
      "    35 |     7350 |     15 |  0.000000E+00 |             f\n",
      "    36 |     7560 |     15 |  0.000000E+00 |             f\n",
      "    37 |     7770 |     15 |  0.000000E+00 |             f\n",
      "    38 |     7980 |     15 |  0.000000E+00 |             f\n",
      "    39 |     8190 |     15 |  0.000000E+00 |             f\n",
      "    40 |     8400 |     15 |  0.000000E+00 |             f\n",
      "    41 |     8610 |     15 |  0.000000E+00 |             f\n",
      "    42 |     8820 |     15 |  0.000000E+00 |             f\n",
      "    43 |     9030 |     15 |  0.000000E+00 |             f\n",
      "    44 |     9240 |     15 |  0.000000E+00 |             f\n",
      "    45 |     9450 |     15 |  0.000000E+00 |             f\n",
      "    46 |     9660 |     15 |  0.000000E+00 |             f\n",
      "    47 |     9870 |     15 |  0.000000E+00 |             f\n",
      "    48 |    10080 |     15 |  0.000000E+00 |             f\n",
      "    49 |    10290 |     15 |  0.000000E+00 |             f\n",
      "    50 |    10500 |     15 |  0.000000E+00 |             f\n",
      "    51 |    10710 |     15 |  0.000000E+00 |             f\n",
      "    52 |    10920 |     15 |  0.000000E+00 |             f\n",
      "    53 |    11130 |     15 |  0.000000E+00 |             f\n",
      "    54 |    11340 |     15 |  0.000000E+00 |             f\n",
      "    55 |    11550 |     15 |  0.000000E+00 |             f\n",
      "    56 |    11760 |     15 |  0.000000E+00 |             f\n",
      "    57 |    11970 |     15 |  0.000000E+00 |             f\n",
      "    58 |    12180 |     15 |  0.000000E+00 |             f\n",
      "    59 |    12390 |     15 |  0.000000E+00 |             f\n",
      "    60 |    12600 |     15 |  0.000000E+00 |             f\n",
      "    61 |    12810 |     15 |  0.000000E+00 |             f\n",
      "    62 |    13020 |     15 |  0.000000E+00 |             f\n",
      "    63 |    13230 |     15 |  0.000000E+00 |             f\n",
      "    64 |    13440 |     15 |  0.000000E+00 |             f\n",
      "    65 |    13650 |     15 |  0.000000E+00 |             f\n",
      "    66 |    13860 |     15 |  0.000000E+00 |             f\n",
      "    67 |    14070 |     15 |  0.000000E+00 |             f\n",
      "    68 |    14280 |     15 |  0.000000E+00 |             f\n",
      "    69 |    14490 |     15 |  0.000000E+00 |             f\n",
      "    70 |    14700 |     15 |  0.000000E+00 |             f\n",
      "    71 |    14910 |     15 |  0.000000E+00 |             f\n",
      "    72 |    15120 |     15 |  0.000000E+00 |             f\n",
      "    73 |    15330 |     15 |  0.000000E+00 |             f\n",
      "    74 |    15540 |     15 |  0.000000E+00 |             f\n",
      "    75 |    15750 |     15 |  0.000000E+00 |             f\n",
      "    76 |    15960 |     15 |  0.000000E+00 |             f\n",
      "    77 |    16170 |     15 |  0.000000E+00 |             f\n",
      "    78 |    16380 |     15 |  0.000000E+00 |             f\n",
      "    79 |    16590 |     15 |  0.000000E+00 |             f\n",
      "    80 |    16800 |     15 |  0.000000E+00 |             f\n",
      "    81 |    17010 |     15 |  0.000000E+00 |             f\n",
      "    82 |    17220 |     15 |  0.000000E+00 |             f\n",
      "    83 |    17430 |     15 |  0.000000E+00 |             f\n",
      "    84 |    17640 |     15 |  0.000000E+00 |             f\n",
      "    85 |    17850 |     15 |  0.000000E+00 |             f\n",
      "    86 |    18060 |     15 |  0.000000E+00 |             f\n",
      "    87 |    18270 |     15 |  0.000000E+00 |             f\n",
      "    88 |    18480 |     15 |  0.000000E+00 |             f\n",
      "    89 |    18690 |     15 |  0.000000E+00 |             f\n",
      "    90 |    18900 |     15 |  0.000000E+00 |             f\n",
      "    91 |    19110 |     15 |  0.000000E+00 |             f\n",
      "    92 |    19320 |     15 |  0.000000E+00 |             f\n",
      "    93 |    19530 |     15 |  0.000000E+00 |             f\n",
      "    94 |    19740 |     15 |  0.000000E+00 |             f\n",
      "    95 |    19950 |     15 |  0.000000E+00 |             f\n",
      "    96 |    20160 |     15 |  0.000000E+00 |             f\n",
      "    97 |    20370 |     15 |  0.000000E+00 |             f\n",
      "    98 |    20580 |     15 |  0.000000E+00 |             f\n",
      "    99 |    20790 |     15 |  0.000000E+00 |             f\n",
      "   100 |    21000 |     15 |  0.000000E+00 |             f\n",
      "Solution 1: Selected models: ['CatBoost', 'GBDT']\n",
      "Solution 2: Selected models: ['AdaBoost', 'CatBoost', 'GBDT', 'Logistic Regression', 'XGBoost']\n",
      "Solution 3: Selected models: ['AdaBoost', 'CatBoost', 'Logistic Regression']\n",
      "Solution 4: Selected models: ['AdaBoost', 'CatBoost', 'GBDT']\n",
      "Solution 5: Selected models: ['AdaBoost', 'CatBoost', 'XGBoost']\n",
      "Solution 6: Selected models: ['AdaBoost', 'CatBoost']\n",
      "Solution 7: Selected models: ['Logistic Regression']\n",
      "Solution 8: Selected models: ['CatBoost']\n",
      "Solution 9: Selected models: ['CatBoost', 'Logistic Regression']\n",
      "Solution 10: Selected models: ['GBDT']\n",
      "Solution 11: Selected models: ['AdaBoost']\n",
      "Solution 12: Selected models: ['GBDT', 'Logistic Regression']\n",
      "Solution 13: Selected models: ['AdaBoost', 'GBDT', 'LightGBM', 'Logistic Regression']\n",
      "Solution 14: Selected models: ['AdaBoost', 'CatBoost', 'GBDT', 'LightGBM', 'Logistic Regression', 'Random Forest']\n",
      "Solution 15: Selected models: ['AdaBoost', 'GBDT']\n",
      "Best solution: Selected models: ['CatBoost', 'GBDT']\n"
     ]
    }
   ],
   "source": [
    "# NSGA3\n",
    "from pymoo.core.problem import ElementwiseProblem\n",
    "from pymoo.algorithms.moo.nsga3 import NSGA3\n",
    "from pymoo.util.ref_dirs import get_reference_directions\n",
    "from pymoo.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# 提取性能指标和模型\n",
    "objectives = [\"AUC\", \"AUPRC\", \"Log-Loss\", \"MCC\"]\n",
    "datasets = performance_results[\"Dataset\"].unique()\n",
    "models = performance_results[\"Model\"].unique()\n",
    "\n",
    "# 数据预处理为字典形式\n",
    "data_dict = {\n",
    "    dataset: performance_results[performance_results[\"Dataset\"] == dataset]\n",
    "    for dataset in datasets\n",
    "}\n",
    "\n",
    "# 定义多目标优化问题\n",
    "class ModelSelectionProblem(ElementwiseProblem):\n",
    "    def __init__(self, data_dict, models, objectives):\n",
    "        self.data_dict = data_dict\n",
    "        self.models = models\n",
    "        self.objectives = objectives\n",
    "        n_var = len(models)  # 决策变量（模型数量）\n",
    "        n_obj = len(objectives)  # 目标数量\n",
    "        super().__init__(n_var=n_var, n_obj=n_obj, xl=0, xu=1, type_var=float)  # 使用浮点型变量\n",
    "\n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        x_binary = np.round(x).astype(int)  # 将浮点数转换为二进制选择\n",
    "        selected_models = [self.models[i] for i, selected in enumerate(x_binary) if selected == 1]\n",
    "\n",
    "        if not selected_models:\n",
    "            # 如果未选中任何模型，返回最差性能\n",
    "            out[\"F\"] = [1.0] * len(self.objectives)  # 所有目标最差性能\n",
    "            return\n",
    "\n",
    "        scores = []\n",
    "        for obj in self.objectives:\n",
    "            obj_scores = []\n",
    "            for dataset, data in self.data_dict.items():\n",
    "                # 筛选选中模型\n",
    "                selected_data = data[data[\"Model\"].isin(selected_models)]\n",
    "                if selected_data.empty:\n",
    "                    obj_scores.append(1.0)  # 无选中模型，性能为最差\n",
    "                else:\n",
    "                    # 计算选中模型的平均性能并取负值\n",
    "                    obj_scores.append(-selected_data[obj].mean())\n",
    "            scores.append(np.mean(obj_scores))  # 对所有数据集取平均\n",
    "        out[\"F\"] = scores\n",
    "\n",
    "# 定义优化问题\n",
    "problem = ModelSelectionProblem(data_dict, models, objectives)\n",
    "\n",
    "# 设置参考方向和算法\n",
    "ref_dirs = get_reference_directions(\"das-dennis\", n_dim=len(objectives), n_partitions=6)\n",
    "algorithm = NSGA3(pop_size=210, ref_dirs=ref_dirs)\n",
    "\n",
    "# 执行优化\n",
    "res = minimize(problem, algorithm, ('n_gen', 100), seed=42, verbose=True)\n",
    "\n",
    "# 输出 Pareto 解\n",
    "optimal_solutions = res.X\n",
    "for i, solution in enumerate(optimal_solutions):\n",
    "    selected_models = [models[j] for j in range(len(models)) if np.round(solution[j]) == 1]\n",
    "    print(f\"Solution {i + 1}: Selected models: {selected_models}\")\n",
    "\n",
    "# 选择最佳解\n",
    "best_solution = optimal_solutions[0]\n",
    "selected_models = [models[j] for j in range(len(models)) if np.round(best_solution[j]) == 1]\n",
    "print(f\"Best solution: Selected models: {selected_models}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |      100 |     13 |             - |             -\n",
      "     2 |      200 |     12 |  0.3706612911 |         ideal\n",
      "     3 |      300 |     13 |  0.0592587235 |             f\n",
      "     4 |      400 |     15 |  0.0143454623 |         ideal\n",
      "     5 |      500 |     13 |  0.1763030078 |         nadir\n",
      "     6 |      600 |     17 |  0.0588656050 |         ideal\n",
      "     7 |      700 |     17 |  0.0097476816 |             f\n",
      "     8 |      800 |     17 |  0.000000E+00 |             f\n",
      "     9 |      900 |     17 |  0.0107948959 |             f\n",
      "    10 |     1000 |     17 |  0.0088023227 |             f\n",
      "    11 |     1100 |     17 |  0.2211944511 |         ideal\n",
      "    12 |     1200 |     17 |  0.000000E+00 |             f\n",
      "    13 |     1300 |     17 |  0.000000E+00 |             f\n",
      "    14 |     1400 |     17 |  0.000000E+00 |             f\n",
      "    15 |     1500 |     17 |  0.000000E+00 |             f\n",
      "    16 |     1600 |     17 |  0.000000E+00 |             f\n",
      "    17 |     1700 |     17 |  0.000000E+00 |             f\n",
      "    18 |     1800 |     17 |  0.000000E+00 |             f\n",
      "    19 |     1900 |     17 |  0.000000E+00 |             f\n",
      "    20 |     2000 |     17 |  0.000000E+00 |             f\n",
      "    21 |     2100 |     17 |  0.000000E+00 |             f\n",
      "    22 |     2200 |     17 |  0.000000E+00 |             f\n",
      "    23 |     2300 |     17 |  0.000000E+00 |             f\n",
      "    24 |     2400 |     17 |  0.000000E+00 |             f\n",
      "    25 |     2500 |     17 |  0.000000E+00 |             f\n",
      "    26 |     2600 |     17 |  0.000000E+00 |             f\n",
      "    27 |     2700 |     17 |  0.000000E+00 |             f\n",
      "    28 |     2800 |     17 |  0.000000E+00 |             f\n",
      "    29 |     2900 |     17 |  0.000000E+00 |             f\n",
      "    30 |     3000 |     17 |  0.000000E+00 |             f\n",
      "    31 |     3100 |     17 |  0.000000E+00 |             f\n",
      "    32 |     3200 |     17 |  0.000000E+00 |             f\n",
      "    33 |     3300 |     17 |  0.000000E+00 |             f\n",
      "    34 |     3400 |     17 |  0.000000E+00 |             f\n",
      "    35 |     3500 |     17 |  0.000000E+00 |             f\n",
      "    36 |     3600 |     17 |  0.000000E+00 |             f\n",
      "    37 |     3700 |     17 |  0.000000E+00 |             f\n",
      "    38 |     3800 |     17 |  0.000000E+00 |             f\n",
      "    39 |     3900 |     17 |  0.000000E+00 |             f\n",
      "    40 |     4000 |     17 |  0.000000E+00 |             f\n",
      "    41 |     4100 |     17 |  0.000000E+00 |             f\n",
      "    42 |     4200 |     17 |  0.000000E+00 |             f\n",
      "    43 |     4300 |     17 |  0.000000E+00 |             f\n",
      "    44 |     4400 |     18 |  0.0628750995 |         nadir\n",
      "    45 |     4500 |     17 |  0.0670936174 |         nadir\n",
      "    46 |     4600 |     17 |  0.000000E+00 |             f\n",
      "    47 |     4700 |     17 |  0.000000E+00 |             f\n",
      "    48 |     4800 |     17 |  0.000000E+00 |             f\n",
      "    49 |     4900 |     17 |  0.000000E+00 |             f\n",
      "    50 |     5000 |     17 |  0.000000E+00 |             f\n",
      "    51 |     5100 |     17 |  0.000000E+00 |             f\n",
      "    52 |     5200 |     17 |  0.000000E+00 |             f\n",
      "    53 |     5300 |     17 |  0.000000E+00 |             f\n",
      "    54 |     5400 |     17 |  0.000000E+00 |             f\n",
      "    55 |     5500 |     17 |  0.000000E+00 |             f\n",
      "    56 |     5600 |     17 |  0.000000E+00 |             f\n",
      "    57 |     5700 |     17 |  0.000000E+00 |             f\n",
      "    58 |     5800 |     17 |  0.000000E+00 |             f\n",
      "    59 |     5900 |     17 |  0.000000E+00 |             f\n",
      "    60 |     6000 |     17 |  0.000000E+00 |             f\n",
      "    61 |     6100 |     17 |  0.000000E+00 |             f\n",
      "    62 |     6200 |     17 |  0.000000E+00 |             f\n",
      "    63 |     6300 |     17 |  0.000000E+00 |             f\n",
      "    64 |     6400 |     17 |  0.000000E+00 |             f\n",
      "    65 |     6500 |     17 |  0.000000E+00 |             f\n",
      "    66 |     6600 |     17 |  0.000000E+00 |             f\n",
      "    67 |     6700 |     17 |  0.000000E+00 |             f\n",
      "    68 |     6800 |     17 |  0.000000E+00 |             f\n",
      "    69 |     6900 |     17 |  0.000000E+00 |             f\n",
      "    70 |     7000 |     17 |  0.000000E+00 |             f\n",
      "    71 |     7100 |     17 |  0.000000E+00 |             f\n",
      "    72 |     7200 |     17 |  0.000000E+00 |             f\n",
      "    73 |     7300 |     17 |  0.000000E+00 |             f\n",
      "    74 |     7400 |     17 |  0.000000E+00 |             f\n",
      "    75 |     7500 |     17 |  0.000000E+00 |             f\n",
      "    76 |     7600 |     17 |  0.000000E+00 |             f\n",
      "    77 |     7700 |     17 |  0.000000E+00 |             f\n",
      "    78 |     7800 |     17 |  0.000000E+00 |             f\n",
      "    79 |     7900 |     17 |  0.000000E+00 |             f\n",
      "    80 |     8000 |     17 |  0.000000E+00 |             f\n",
      "    81 |     8100 |     17 |  0.000000E+00 |             f\n",
      "    82 |     8200 |     17 |  0.000000E+00 |             f\n",
      "    83 |     8300 |     17 |  0.000000E+00 |             f\n",
      "    84 |     8400 |     17 |  0.000000E+00 |             f\n",
      "    85 |     8500 |     17 |  0.000000E+00 |             f\n",
      "    86 |     8600 |     17 |  0.000000E+00 |             f\n",
      "    87 |     8700 |     17 |  0.000000E+00 |             f\n",
      "    88 |     8800 |     17 |  0.000000E+00 |             f\n",
      "    89 |     8900 |     17 |  0.000000E+00 |             f\n",
      "    90 |     9000 |     17 |  0.000000E+00 |             f\n",
      "    91 |     9100 |     17 |  0.000000E+00 |             f\n",
      "    92 |     9200 |     17 |  0.000000E+00 |             f\n",
      "    93 |     9300 |     17 |  0.000000E+00 |             f\n",
      "    94 |     9400 |     17 |  0.000000E+00 |             f\n",
      "    95 |     9500 |     17 |  0.000000E+00 |             f\n",
      "    96 |     9600 |     17 |  0.000000E+00 |             f\n",
      "    97 |     9700 |     17 |  0.000000E+00 |             f\n",
      "    98 |     9800 |     17 |  0.000000E+00 |             f\n",
      "    99 |     9900 |     17 |  0.000000E+00 |             f\n",
      "   100 |    10000 |     18 |  0.0142101826 |             f\n",
      "Selected models based on maximum AUC: [0.15133494 0.21429656 0.11949444 0.79274567 0.17384747 0.02096688\n",
      " 0.92690726 0.4502141  0.49362052]\n",
      "Best model with highest probability: Random Forest\n"
     ]
    }
   ],
   "source": [
    "# NSGA3\n",
    "from pymoo.core.problem import ElementwiseProblem\n",
    "from pymoo.algorithms.moo.nsga3 import NSGA3\n",
    "from pymoo.util.ref_dirs import get_reference_directions\n",
    "from pymoo.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# 提取性能指标和模型\n",
    "objectives = [\"AUC\", \"AUPRC\", \"Log-Loss\", \"MCC\"]\n",
    "datasets = performance_results[\"Dataset\"].unique()\n",
    "models = performance_results[\"Model\"].unique()\n",
    "\n",
    "# 数据预处理为字典形式\n",
    "data_dict = {\n",
    "    dataset: performance_results[performance_results[\"Dataset\"] == dataset]\n",
    "    for dataset in datasets\n",
    "}\n",
    "\n",
    "# 定义多目标优化问题\n",
    "class ModelSelectionProblem(ElementwiseProblem):\n",
    "    def __init__(self, data_dict, models, objectives):\n",
    "        self.data_dict = data_dict\n",
    "        self.models = models\n",
    "        self.objectives = objectives\n",
    "        n_var = len(models)  # 决策变量（模型数量）\n",
    "        n_obj = len(objectives)  # 目标数量\n",
    "        super().__init__(n_var=n_var, n_obj=n_obj, xl=0, xu=1, type_var=float)  # 使用浮点型变量\n",
    "\n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        x_binary = np.round(x).astype(int)  # 将浮点数转换为二进制选择\n",
    "        selected_models = [self.models[i] for i, selected in enumerate(x_binary) if selected == 1]\n",
    "\n",
    "        if not selected_models:\n",
    "            # 如果未选中任何模型，返回最差性能\n",
    "            out[\"F\"] = [1.0] * len(self.objectives)  # 所有目标最差性能\n",
    "            return\n",
    "\n",
    "        scores = []\n",
    "        for obj in self.objectives:\n",
    "            obj_scores = []\n",
    "            for dataset, data in self.data_dict.items():\n",
    "                # 筛选选中模型\n",
    "                selected_data = data[data[\"Model\"].isin(selected_models)]\n",
    "                if selected_data.empty:\n",
    "                    obj_scores.append(1.0)  # 无选中模型，性能为最差\n",
    "                else:\n",
    "                    # 计算选中模型的平均性能并取负值\n",
    "                    obj_scores.append(-selected_data[obj].mean())\n",
    "            scores.append(np.mean(obj_scores))  # 对所有数据集取平均\n",
    "        out[\"F\"] = scores\n",
    "\n",
    "# 定义优化问题\n",
    "problem = ModelSelectionProblem(data_dict, models, objectives)\n",
    "\n",
    "# 设置参考方向和算法\n",
    "ref_dirs = get_reference_directions(\"das-dennis\", n_dim=len(objectives), n_partitions=6)\n",
    "algorithm = NSGA3(pop_size=100, ref_dirs=ref_dirs)\n",
    "\n",
    "# 执行优化\n",
    "res = minimize(problem, algorithm, ('n_gen', 100), seed=42, verbose=True)\n",
    "\n",
    "# 输出 Pareto 解\n",
    "optimal_solutions = res.X\n",
    "optimal_objectives = res.F  # Pareto 前沿解的目标值\n",
    "\n",
    "# 获取所有解的 AUC 值，并按 AUC 排序\n",
    "auc_values = optimal_objectives[:, 0]  # 假设 AUC 是第一个目标（根据 objectives 列表中的顺序）\n",
    "best_solution_index = np.argmin(auc_values)  # 找到最大 AUC 的解的索引\n",
    "\n",
    "# 获取对应 AUC 最大解的所有模型选择概率\n",
    "best_solution = optimal_solutions[best_solution_index]\n",
    "\n",
    "# 输出当前解的模型概率\n",
    "print(f\"Selected models based on maximum AUC: {best_solution}\")\n",
    "\n",
    "# 在这些模型中选择概率最高的模型\n",
    "max_prob_model_index = np.argmax(best_solution)  # 选择最大概率的模型索引\n",
    "selected_model = models[max_prob_model_index]  # 获取模型名称\n",
    "\n",
    "print(f\"Best model with highest probability: {selected_model}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |      100 |      1 |             - |             -\n",
      "     2 |      200 |      1 |  0.000000E+00 |             f\n",
      "     3 |      300 |      1 |  0.000000E+00 |             f\n",
      "     4 |      400 |      1 |  0.000000E+00 |             f\n",
      "     5 |      500 |      1 |  0.000000E+00 |             f\n",
      "     6 |      600 |      1 |  0.000000E+00 |             f\n",
      "     7 |      700 |      1 |  0.000000E+00 |             f\n",
      "     8 |      800 |      1 |  0.000000E+00 |             f\n",
      "     9 |      900 |      1 |  0.000000E+00 |             f\n",
      "    10 |     1000 |      1 |  0.000000E+00 |             f\n",
      "    11 |     1100 |      1 |  0.000000E+00 |             f\n",
      "    12 |     1200 |      1 |  0.000000E+00 |             f\n",
      "    13 |     1300 |      1 |  0.000000E+00 |             f\n",
      "    14 |     1400 |      1 |  0.000000E+00 |             f\n",
      "    15 |     1500 |      1 |  0.000000E+00 |             f\n",
      "    16 |     1600 |      1 |  0.000000E+00 |             f\n",
      "    17 |     1700 |      1 |  0.000000E+00 |             f\n",
      "    18 |     1800 |      1 |  0.000000E+00 |             f\n",
      "    19 |     1900 |      1 |  0.000000E+00 |             f\n",
      "    20 |     2000 |      1 |  0.000000E+00 |             f\n",
      "    21 |     2100 |      1 |  0.000000E+00 |             f\n",
      "    22 |     2200 |      1 |  0.000000E+00 |             f\n",
      "    23 |     2300 |      1 |  0.000000E+00 |             f\n",
      "    24 |     2400 |      1 |  0.000000E+00 |             f\n",
      "    25 |     2500 |      1 |  0.000000E+00 |             f\n",
      "    26 |     2600 |      1 |  0.000000E+00 |             f\n",
      "    27 |     2700 |      1 |  0.000000E+00 |             f\n",
      "    28 |     2800 |      1 |  0.000000E+00 |             f\n",
      "    29 |     2900 |      1 |  0.000000E+00 |             f\n",
      "    30 |     3000 |      1 |  0.000000E+00 |             f\n",
      "    31 |     3100 |      1 |  0.000000E+00 |             f\n",
      "    32 |     3200 |      1 |  0.000000E+00 |             f\n",
      "    33 |     3300 |      1 |  0.000000E+00 |             f\n",
      "    34 |     3400 |      1 |  0.000000E+00 |             f\n",
      "    35 |     3500 |      1 |  0.000000E+00 |             f\n",
      "    36 |     3600 |      1 |  0.000000E+00 |             f\n",
      "    37 |     3700 |      1 |  0.000000E+00 |             f\n",
      "    38 |     3800 |      1 |  0.000000E+00 |             f\n",
      "    39 |     3900 |      1 |  0.000000E+00 |             f\n",
      "    40 |     4000 |      1 |  0.000000E+00 |             f\n",
      "    41 |     4100 |      1 |  0.000000E+00 |             f\n",
      "    42 |     4200 |      1 |  0.000000E+00 |             f\n",
      "    43 |     4300 |      1 |  0.000000E+00 |             f\n",
      "    44 |     4400 |      1 |  0.000000E+00 |             f\n",
      "    45 |     4500 |      1 |  0.000000E+00 |             f\n",
      "    46 |     4600 |      1 |  0.000000E+00 |             f\n",
      "    47 |     4700 |      1 |  0.000000E+00 |             f\n",
      "    48 |     4800 |      1 |  0.000000E+00 |             f\n",
      "    49 |     4900 |      1 |  0.000000E+00 |             f\n",
      "    50 |     5000 |      1 |  0.000000E+00 |             f\n",
      "    51 |     5100 |      1 |  0.000000E+00 |             f\n",
      "    52 |     5200 |      1 |  0.000000E+00 |             f\n",
      "    53 |     5300 |      1 |  0.000000E+00 |             f\n",
      "    54 |     5400 |      1 |  0.000000E+00 |             f\n",
      "    55 |     5500 |      1 |  0.000000E+00 |             f\n",
      "    56 |     5600 |      1 |  0.000000E+00 |             f\n",
      "    57 |     5700 |      1 |  0.000000E+00 |             f\n",
      "    58 |     5800 |      1 |  0.000000E+00 |             f\n",
      "    59 |     5900 |      1 |  0.000000E+00 |             f\n",
      "    60 |     6000 |      1 |  0.000000E+00 |             f\n",
      "    61 |     6100 |      1 |  0.000000E+00 |             f\n",
      "    62 |     6200 |      1 |  0.000000E+00 |             f\n",
      "    63 |     6300 |      1 |  0.000000E+00 |             f\n",
      "    64 |     6400 |      1 |  0.000000E+00 |             f\n",
      "    65 |     6500 |      1 |  0.000000E+00 |             f\n",
      "    66 |     6600 |      1 |  0.000000E+00 |             f\n",
      "    67 |     6700 |      1 |  0.000000E+00 |             f\n",
      "    68 |     6800 |      1 |  0.000000E+00 |             f\n",
      "    69 |     6900 |      1 |  0.000000E+00 |             f\n",
      "    70 |     7000 |      1 |  0.000000E+00 |             f\n",
      "    71 |     7100 |      1 |  0.000000E+00 |             f\n",
      "    72 |     7200 |      1 |  0.000000E+00 |             f\n",
      "    73 |     7300 |      1 |  0.000000E+00 |             f\n",
      "    74 |     7400 |      1 |  0.000000E+00 |             f\n",
      "    75 |     7500 |      1 |  0.000000E+00 |             f\n",
      "    76 |     7600 |      1 |  0.000000E+00 |             f\n",
      "    77 |     7700 |      1 |  0.000000E+00 |             f\n",
      "    78 |     7800 |      1 |  0.000000E+00 |             f\n",
      "    79 |     7900 |      1 |  0.000000E+00 |             f\n",
      "    80 |     8000 |      1 |  0.000000E+00 |             f\n",
      "    81 |     8100 |      1 |  0.000000E+00 |             f\n",
      "    82 |     8200 |      1 |  0.000000E+00 |             f\n",
      "    83 |     8300 |      1 |  0.000000E+00 |             f\n",
      "    84 |     8400 |      1 |  0.000000E+00 |             f\n",
      "    85 |     8500 |      1 |  0.000000E+00 |             f\n",
      "    86 |     8600 |      1 |  0.000000E+00 |             f\n",
      "    87 |     8700 |      1 |  0.000000E+00 |             f\n",
      "    88 |     8800 |      1 |  0.000000E+00 |             f\n",
      "    89 |     8900 |      1 |  0.000000E+00 |             f\n",
      "    90 |     9000 |      1 |  0.000000E+00 |             f\n",
      "    91 |     9100 |      1 |  0.000000E+00 |             f\n",
      "    92 |     9200 |      1 |  0.000000E+00 |             f\n",
      "    93 |     9300 |      1 |  0.000000E+00 |             f\n",
      "    94 |     9400 |      1 |  0.000000E+00 |             f\n",
      "    95 |     9500 |      1 |  0.000000E+00 |             f\n",
      "    96 |     9600 |      1 |  0.000000E+00 |             f\n",
      "    97 |     9700 |      1 |  0.000000E+00 |             f\n",
      "    98 |     9800 |      1 |  0.000000E+00 |             f\n",
      "    99 |     9900 |      1 |  0.000000E+00 |             f\n",
      "   100 |    10000 |      1 |  0.000000E+00 |             f\n",
      "Selected models based on maximum AUC: [0.37633514 0.08809239 0.29203421 0.0038129  0.3884654  0.26920327\n",
      " 0.59678488 0.14852089 0.48249057]\n",
      "Best model with highest probability: Random Forest\n"
     ]
    }
   ],
   "source": [
    "from pymoo.core.problem import ElementwiseProblem\n",
    "from pymoo.algorithms.moo.nsga3 import NSGA3\n",
    "from pymoo.util.ref_dirs import get_reference_directions\n",
    "from pymoo.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# 提取性能指标和模型\n",
    "objectives = [\"AUC\", \"AUPRC\", \"Log-Loss\"]\n",
    "datasets = performance_results[\"Dataset\"].unique()\n",
    "models = performance_results[\"Model\"].unique()\n",
    "\n",
    "# 数据预处理为字典形式\n",
    "data_dict = {\n",
    "    dataset: performance_results[performance_results[\"Dataset\"] == dataset]\n",
    "    for dataset in datasets\n",
    "}\n",
    "\n",
    "# 定义多目标优化问题\n",
    "class ModelSelectionProblem(ElementwiseProblem):\n",
    "    def __init__(self, data_dict, models, objectives):\n",
    "        self.data_dict = data_dict\n",
    "        self.models = models\n",
    "        self.objectives = objectives\n",
    "        n_var = len(models)  # 决策变量（模型数量）\n",
    "        n_obj = len(objectives)  # 目标数量\n",
    "        super().__init__(n_var=n_var, n_obj=n_obj, xl=0, xu=1, type_var=float)  # 使用浮点型变量\n",
    "\n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        x_binary = np.round(x).astype(int)  # 将浮点数转换为二进制选择\n",
    "        selected_models = [self.models[i] for i, selected in enumerate(x_binary) if selected == 1]\n",
    "\n",
    "        if not selected_models:\n",
    "            # 如果未选中任何模型，返回最差性能\n",
    "            out[\"F\"] = [1.0] * len(self.objectives)  # 所有目标最差性能\n",
    "            return\n",
    "\n",
    "        scores = []\n",
    "        for obj in self.objectives:\n",
    "            obj_scores = []\n",
    "            for dataset, data in self.data_dict.items():\n",
    "                # 筛选选中模型\n",
    "                selected_data = data[data[\"Model\"].isin(selected_models)]\n",
    "                if selected_data.empty:\n",
    "                    obj_scores.append(1.0)  # 无选中模型，性能为最差\n",
    "                else:\n",
    "                    # 计算选中模型的平均性能并取负值（Log-Loss需要最小化）\n",
    "                    if obj == \"Log-Loss\":\n",
    "                        obj_scores.append(selected_data[obj].mean())  # Log-Loss 越小越好，保留正值\n",
    "                    else:\n",
    "                        obj_scores.append(-selected_data[obj].mean())  # AUC, AUPRC 和 MCC 越大越好，取负值\n",
    "            scores.append(np.mean(obj_scores))  # 对所有数据集取平均\n",
    "        out[\"F\"] = scores\n",
    "\n",
    "# 定义优化问题\n",
    "problem = ModelSelectionProblem(data_dict, models, objectives)\n",
    "\n",
    "# 设置参考方向和算法\n",
    "ref_dirs = get_reference_directions(\"das-dennis\", n_dim=len(objectives), n_partitions=6)\n",
    "algorithm = NSGA3(pop_size=100, ref_dirs=ref_dirs)\n",
    "\n",
    "# 执行优化\n",
    "res = minimize(problem, algorithm, ('n_gen', 100), seed=42, verbose=True)\n",
    "\n",
    "# 输出 Pareto 解\n",
    "optimal_solutions = res.X\n",
    "optimal_objectives = res.F  # Pareto 前沿解的目标值\n",
    "\n",
    "# 获取所有解的 AUC 值，并按 AUC 排序\n",
    "auc_values = optimal_objectives[:, 0]  # 假设 AUC 是第一个目标（根据 objectives 列表中的顺序）\n",
    "best_solution_index = np.argmin(auc_values)  # 找到最大 AUC 的解的索引\n",
    "\n",
    "# 获取对应 AUC 最大解的所有模型选择概率\n",
    "best_solution = optimal_solutions[best_solution_index]\n",
    "\n",
    "# 输出当前解的模型概率\n",
    "print(f\"Selected models based on maximum AUC: {best_solution}\")\n",
    "\n",
    "# 在这些模型中选择概率最高的模型\n",
    "max_prob_model_index = np.argmax(best_solution)  # 选择最大概率的模型索引\n",
    "selected_model = models[max_prob_model_index]  # 获取模型名称\n",
    "\n",
    "print(f\"Best model with highest probability: {selected_model}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
